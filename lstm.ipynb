{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hao1zhao/Model/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3985f6",
      "metadata": {
        "origin_pos": 0,
        "id": "7f3985f6"
      },
      "source": [
        "# LSTM\n",
        "\n",
        "$\\mathbf{I}_t$:Input $\\mathbf{F}_t$:forget $\\mathbf{O}_t$:output $\\tilde{\\mathbf{C}}_t$:candidate memory cell $\\mathbf{H}_t$:memory cell\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
        "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n",
        "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n",
        "\\end{aligned}\n",
        "$$\n",
        "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),$$\n",
        "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "Rearw5I1PGnE"
      },
      "id": "Rearw5I1PGnE",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initial parameter\n",
        "bs,T,i_size,h_size =2,3,4,5 #batch_size,time,input_size,hidden_size\n",
        "input = torch.randn(bs,T,i_size)\n",
        "c0 = torch.randn(bs,h_size)\n",
        "h0 = torch.randn(bs,h_size)\n",
        "#API\n",
        "lstm_layer = nn.LSTM(i_size,h_size,batch_first=True)\n",
        "output,(h_final,c_final) = lstm_layer(input,(h0.unsqueeze(0),c0.unsqueeze(0)))\n",
        "for k,v in lstm_layer.named_parameters():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAwFztoTPGpn",
        "outputId": "d3e32896-af3d-45c2-df93-b8e5a2d37528"
      },
      "id": "OAwFztoTPGpn",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight_ih_l0 torch.Size([20, 4])\n",
            "weight_hh_l0 torch.Size([20, 5])\n",
            "bias_ih_l0 torch.Size([20])\n",
            "bias_hh_l0 torch.Size([20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unidirection LSTM\n",
        "def listm_forward(input,initial_states,w_ih,w_hh,b_ih,b_hh):\n",
        "  h0,c0 = initial_states\n",
        "  bs,T,i_size = input.shape #break down input\n",
        "  h_size = w_ih.shape[0]//4\n",
        "\n",
        "  prev_h = h0\n",
        "  prev_c = c0\n",
        "  #w_ih {4*h_size,i_size}\n",
        "  #w_hh {4*h_size,h_size}\n",
        "  batch_w_ih = w_ih.unsqueeze(0).tile(bs,1,1) #{bs,4*h_size,i_size}\n",
        "  batch_w_hh = w_hh.unsqueeze(0).tile(bs,1,1) #{bs,4*h_size,h_size}\n",
        "\n",
        "  output_size = h_size\n",
        "  output = torch.zeros(bs,T,output_size)\n",
        "  for t in range(T):\n",
        "    x = input[:,t,:] #iterate T, {bs,i_size}\n",
        "    w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)) #{bs,4*h_size,1}\n",
        "    w_times_x = w_times_x.squeeze(-1) #{bs,4*h_size}\n",
        "\n",
        "    w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)) #{bs,4*h_size,1}\n",
        "    w_times_h_prev = w_times_h_prev.squeeze(-1) #{bs,4*h_size}\n",
        "\n",
        "    #gates\n",
        "    i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size] +b_ih[:h_size] + b_hh[:h_size])\n",
        "    f_t = torch.sigmoid(w_times_x[:,h_size:2*h_size] + w_times_h_prev[:,h_size:2*h_size] +b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
        "    cc_t = torch.tanh(w_times_x[:,2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size] +b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])\n",
        "    o_t = torch.sigmoid(w_times_x[:,3*h_size:4*h_size] + w_times_h_prev[:,3*h_size:4*h_size] +b_ih[3*h_size:4*h_size] + b_hh[3*h_size:4*h_size])\n",
        "    #update c and h\n",
        "    prev_c = f_t*prev_c + i_t*cc_t\n",
        "    prev_h = o_t*torch.tanh(prev_c)\n",
        "    \n",
        "    output[:,t,:] = prev_h\n",
        "\n",
        "  return output, (prev_h,prev_c)\n",
        "#test\n",
        "output_custom,(h_final_custom,c_final_custom) = listm_forward(input,(h0,c0),lstm_layer.weight_ih_l0,lstm_layer.weight_hh_l0,lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0)\n",
        "print(f'output:{output}',f'output_custom:{output_custom}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPFyU8vyWUeH",
        "outputId": "5d234db7-e712-407d-9a8e-b154f9279eb5"
      },
      "id": "SPFyU8vyWUeH",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output:tensor([[[-0.1076, -0.4109,  0.0216, -0.4851,  0.3007],\n",
            "         [ 0.0926, -0.3760,  0.0407, -0.2144,  0.3392],\n",
            "         [ 0.2056, -0.2599, -0.0100, -0.1721,  0.2170]],\n",
            "\n",
            "        [[-0.1218, -0.1491, -0.0662, -0.0635,  0.1405],\n",
            "         [-0.0518, -0.0886,  0.0478, -0.1821,  0.0289],\n",
            "         [ 0.1393, -0.0395, -0.0270, -0.1378,  0.1511]]],\n",
            "       grad_fn=<TransposeBackward0>) output_custom:tensor([[[-0.1076, -0.4109,  0.0216, -0.4851,  0.3007],\n",
            "         [ 0.0926, -0.3760,  0.0407, -0.2144,  0.3392],\n",
            "         [ 0.2056, -0.2599, -0.0100, -0.1721,  0.2170]],\n",
            "\n",
            "        [[-0.1218, -0.1491, -0.0662, -0.0635,  0.1405],\n",
            "         [-0.0518, -0.0886,  0.0478, -0.1821,  0.0289],\n",
            "         [ 0.1393, -0.0395, -0.0270, -0.1378,  0.1511]]], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}